# Imen Ferchichi, Francesca Pjetri, Axel Zaccheo
# Programming in Finance and Economics, USI University in Lugano, Switzerland
# Final project, end of Fall semester 2024/2025, Master in Finance
# imen.ferchichi@usi.ch; francesca.pjetri@usi.ch; axel.zaccheo@usi.ch

## ============================= PART A ================================== ## 
## TASK:
## We need to find the CIK code of the S&P 100 companies

# Install beautifulsoup 4 and Wikipedia
pip install requests beautifulsoup4 pandas
pip install wikipedia

# Import all necessary packages
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import numpy as np
import pandas as pd
import wikipedia as wp
import yfinance as yf
import requests 
import os
import re
import requests
import unicodedata
import csv
from bs4 import BeautifulSoup
from datetime import datetime


SP100AnalysiswithGNNs
@author: timothewt
# https://github.com/timothewt/SP100AnalysisWithGNNs/blob/master/notebooks/1-data_collection_and_preprocessing.ipynb

# Import all S&P 100 companies Ticker, Name and Sector

html = wp.page("S&P 100").html().encode("UTF-8")
stocks = pd.read_html(html)[2].set_index("Symbol")
stocks = stocks.drop("GOOG")
stocks.loc["GOOGL", "Name"] = "Alphabet"
stocks = stocks.rename(index={"BRK.B": "BRK-B"})
print(stocks)
# end of the taken code from timothewt

# Extract tickers from first column and create a list with quotation marks
tickers_sp100 = stocks.index.tolist()


SEC Filing Scraper
@author: AdamGetbags
# https://github.com/AdamGetbags/secAPI/blob/main/secFilingScraper.py
# Create request header
headers = {'User-Agent': "axelzaccheo@gmail.com"}
# Get all companies data
companyTickers = requests.get(
    "https://www.sec.gov/files/company_tickers.json",
    headers=headers
    )

# Review response / keys
##Â print(companyTickers.json().keys())

# Format response to dictionary and get first key/value
firstEntry = companyTickers.json()['0']
# Parse CIK // without leading zeros
directCik = companyTickers.json()['0']['cik_str']

# Dictionary to dataframe
companyData = pd.DataFrame.from_dict(companyTickers.json(),
                                     orient='index')

# Add leading zeros to CIK
companyData['cik_str'] = companyData['cik_str'].astype(
                           str).str.zfill(10)
# Review data
print(companyData)
# end of the code taken from AdamGetbags

# Filter DataFrame using the tickers_sp100 list
SP100_CIK_DF = companyData[companyData['ticker'].isin(tickers_sp100)]
# Show result
print(SP100_CIK_DF)

# Extract all the CIK codes from 'cik_str' in the SP_100_CIK_DF DataFrame
cik_list = SP100_CIK_DF['cik_str'].tolist()
# Show the list
print(cik_list)


## ============================= PART B ================================== ## 
## TASK:
## With the cik list we need to extract the 10Q filings of these companies
## in the period 2018-2023

# Now with the cik_list we want to extract the company data in JSON format
# for this part we took the code from our Programming in Finance and Economics classes
# 03.2Python_API
# generate and print URL, we added using a for loop to iterate for each CIK of the S&P 100

for cik in cik_list:
    url = f"https://data.sec.gov/submissions/CIK{str(cik).zfill(10)}.json"
    print(url)

# Name of the file where to save the URL
output_file = "cik_urls.csv"

# These lines of code that follow (until beginning of Part C) are not mandatory 
# but you can store like this all the JSON URL, it was made with the help of ChatGPT

# Write URL in a csv file
with open(output_file, mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(['CIK', 'URL'])  # Header of the CSV

    for cik in cik_list:
        url = f"https://data.sec.gov/submissions/CIK{str(cik).zfill(10)}.json"
        writer.writerow([cik, url])  # Write CIK and URL in the file

print(f"File saved as {output_file}")

## ============================= PART C ================================== ## 

# Input file containing CIK and JSON URLs
input_file = "cik_urls.csv"  # Contains CIK and JSON file URLs
output_file = "10Q_keyword_counts.csv"  # Output file with keyword counts

# Keywords to search for
keywords = ["vaccine", "covid19", "pandemic"]

# Date range for filtering (2018 to 2023)
start_year = 2018
end_year = 2023


# Prepare output file
with open(output_file, mode='w', newline='', encoding='utf-8') as file_out:
    writer = csv.writer(file_out)
    writer.writerow(['CIK', 'Document_URL', 'Filing_Date', 'Vaccine_Count', 'COVID19_Count', 'Pandemic_Count'])  # CSV Header

    # Read CIK and URLs from input CSV file
    with open(input_file, mode='r', newline='', encoding='utf-8') as file_in:
        reader = csv.reader(file_in)
        next(reader)  # Skip header row

        for row in reader:
            cik, json_url = row
            try:
                # Request JSON data for the current company
                response = requests.get(json_url, headers=headers)

                if response.status_code == 200:
                    data = response.json()
                    filings = data['filings']['recent']

                    # Filter for Form 10-Q documents in the desired date range
                    for i, form in enumerate(filings['form']):
                        filing_date = filings['filingDate'][i]
                        filing_year = datetime.strptime(filing_date, "%Y-%m-%d").year

                        if form == "10-Q" and start_year <= filing_year <= end_year:
                            accession_number = filings['accessionNumber'][i]
                            primary_document = filings['primaryDocument'][i]

                            # Construct the URL for the 10-Q document
                            document_url = (
                                f"https://www.sec.gov/Archives/edgar/data/"
                                f"{str(cik).zfill(10)}/"
                                f"{accession_number.replace('-', '')}/{primary_document}"
                            )
                            # Access the 10-Q document and parse its content
                            document_response = requests.get(document_url, headers=headers)
                            if document_response.status_code == 200:
                                soup = BeautifulSoup(document_response.content, "html.parser")

                                # Extract text content and count keyword occurrences
                                text = soup.get_text().lower()  # Convert text to lowercase
                                vaccine_count = text.count("vaccine")
                                covid19_count = text.count("covid19")
                                pandemic_count = text.count("pandemic")

                                # Write the counts to the output CSV file
                                writer.writerow([
                                    cik, document_url, filing_date, vaccine_count, covid19_count, pandemic_count
                                ])
                                print(f"Processed: {document_url} for {filing_date}")
                            else:
                                print(f"Failed to fetch document: {document_url}")
                else:
                    print(f"Failed to fetch data for CIK {cik}. Status Code: {response.status_code}")
            except Exception as e:
                print(f"Error processing CIK {cik}: {e}")

print(f"Keyword counts saved to {output_file}")

# Input file: Output from the previous step
input_file = "10Q_keyword_counts.csv"

# Load the CSV file into a pandas DataFrame
df = pd.read_csv(input_file)

# Group by 'CIK' and aggregate counts for each company
grouped = df.groupby("CIK").agg({
    "Vaccine_Count": "sum",
    "COVID19_Count": "sum",
    "Pandemic_Count": "sum"
}).reset_index()

# Add a new column to count how many companies mention each keyword
grouped['Mentions_Vaccine'] = grouped['Vaccine_Count'] > 0
grouped['Mentions_COVID19'] = grouped['COVID19_Count'] > 0
grouped['Mentions_Pandemic'] = grouped['Pandemic_Count'] > 0

# Summary: Total number of companies that mention each keyword
summary = {
    "Total_Companies": grouped['CIK'].nunique(),
    "Companies_with_Vaccine": grouped['Mentions_Vaccine'].sum(),
    "Companies_with_COVID19": grouped['Mentions_COVID19'].sum(),
    "Companies_with_Pandemic": grouped['Mentions_Pandemic'].sum()
}

# Print the summary
print("Summary of Keyword Mentions:")
for key, value in summary.items():
    print(f"{key}: {value}")

# Create a final summarized DataFrame
summary_df = pd.DataFrame({
    "Keyword": ["Vaccine", "COVID19", "Pandemic"],
    "Companies_Mentioning": [
        grouped['Mentions_Vaccine'].sum(),
        grouped['Mentions_COVID19'].sum(),
        grouped['Mentions_Pandemic'].sum()
    ],
    "Total_Mentions": [
        grouped['Vaccine_Count'].sum(),
        grouped['COVID19_Count'].sum(),
        grouped['Pandemic_Count'].sum()
    ]
})

# Show the summary DataFrame
print("\nSummary DataFrame:")
print(summary_df)

# Optional: Save the summarized data to a CSV file
summary_df.to_csv("keyword_summary.csv", index=False)

# Show detailed mentions per company (if needed)
print("\nDetailed Mentions Per Company:")
print(grouped)

## ============================= PART D ================================== ##

def plots_10q_filings(analysis_results):
    covid_data = []
    pandemic_data = []
    vaccine_data = []

    for result in analysis_results:
        filing_date = pd.to_datetime(result['filingDate'])
        for word, count in result['wordCounts'].items():
            if word.lower() == "covid-19":
                covid_data.append({'date': filing_date, 'count': count})
            elif word.lower() == "pandemic":
                pandemic_data.append({'date': filing_date, 'count': count})
            elif word.lower() == "vaccine":
                vaccine_data.append({'date': filing_date, 'count': count})

    # Convert lists to DataFrames
    covid_df = pd.DataFrame(covid_data)
    pandemic_df = pd.DataFrame(pandemic_data)
    vaccine_df = pd.DataFrame(vaccine_data)

    # Function to plot and save
    def plot_and_save(df, title, filename):
        if not df.empty:
            plt.figure(figsize=(12, 6))
            sns.lineplot(data=df, x="date", y="count")
            plt.title(title)
            plt.xlabel("Date")
            plt.ylabel("Count")
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.savefig(filename, format="png")
            plt.show()
        else:
            print(f"No data available for {title}")

    # Plot for each word
    plot_and_save(covid_df, "Covid-19 count per Quarter", "Covid_count_per_Quarter.png")
    plot_and_save(pandemic_df, "Pandemic count per Quarter", "Pandemic_count_per_Quarter.png")
    plot_and_save(vaccine_df, "Vaccine count per Quarter", "Vaccine_count_per_Quarter.png")
